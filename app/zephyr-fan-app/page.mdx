
import LinkTo from '@/components/LinkTo'
import { Avatar, AvatarImage } from '@/components/ui/avatar'
import Image from 'next/image'

<LinkTo displayText="Back" link="/" />
<br />

# App: Zephyr Fan

[Demo](https://zephyrfan.netlify.app/)

Autonomous Controlled Air Fan App. Using object detection to face people and hand gesture detection to control fan speed and mode of operation.

Built on React Native, TypeScript, Expo, ThreeJS and WebSockets.

<div className="relative w-full h-96 bg-zinc-50 rounded-lg flex justify-center items-center p-12 my-6">
  <video src="/my-video.mp4" width={159} height={100} autoPlay muted loop playsInline className="rounded-lg" />
  <Image src="/phone.png" alt="Project Share Website" width={190} height={50} className="absolute rounded" draggable={false}/>
</div>


## Pre Requisites

We were prompted with: _Creating a robotic system that helps humans._

Thinking on that prompt from the University of Edinburgh's *extremely* hot rooms in AT_5 building it became trivial to think of a smart fan. We decided it would need:

- Built-in camera to track and face people's heads and hand gestures as remote control
- On-board Raspberry Pi to run ML vision models
- App & Website to control the fan
- WebSocket for two-way communication between control loop and the app

<div className="relative w-full bg-zinc-50 rounded-lg my-6 p-4">
  <Image src="/zephyr-day-0.png" alt="Hand drawn sketch of a fan with arrows, showing vertical and horizontal movement, a built in camera and a microcontroller" width={1990} height={2000} className="object-contain w-full h-64 bg-zinc-50 rounded-lg" draggable={false}/>
  <p className="text-xs opacity-50 text-center m-0 mt-2">Day 0: Sketch of the capabilities of the fan. Facing direction, power and mode of operation to be controlled by hand gestures and companion app</p>
</div>

Additionally, as the system would be demoed in a public space, it needed:
- Multi-user support (for demo purposes)
- Admin / View only mode (for demo purposes)
- Dynamically generated QR-code to direct users to the web app

Looking back, one of our smartest moves was to focus on the demo experience from Day 0. It allowed judges to experience the project's capabilities firsthand, directly from their own devices.

<div className="relative w-full bg-zinc-50 rounded-lg my-6 p-4">
  <Image src="/zf-diagram.svg" alt="Technical layout of the project" width={1990} height={2000} className="object-contain w-full bg-zinc-50 rounded-lg" draggable={false}/>
  <p className="text-xs opacity-50 text-center m-0 mt-2">Overview of the project's technical layout</p>
</div>

I focused on: 
- Design and development of the app and website
- Development of the control loop
- Development of the WebSocket connection

As a little bonus, I dipped my toes into 3D modeling and rendering!

As both the designer and developer of the app, I had the freedom to choose the technology stack and design the app's user experience

Next, I'll cover some of the most interesting parts of the design and development process. 

## Design

As a form of challenge and looking for a premium-feeling experience, I decided to include a 3D model of the fan on the app. This idea followed a similar train of thought as what you would see from other premium IoT apps.

<div className="relative w-full bg-zinc-50 rounded-lg my-6 p-4">
  <Image src="/zf-design.png" alt="Screenshots of IoT apps on the left with a screenshot of Zephyr Fan App on the right" width={1990} height={2000} className="object-contain w-full h-64 bg-zinc-50 rounded-lg" draggable={false}/>
  <p className="text-xs opacity-50 text-center m-0 mt-2">Design inspiration of the app</p>
</div> 

Part of committing to a real 3D render, not just an image of a render, was that I could adjust the representation dynamically. I envisioned to tweak the 3D model to be a 1 to 1 render of what the fan is currently doing. For example, it becomes obvious when removing the blade cage, the speed of the fan is represented by the speed of the blades!

<div className="relative w-full h-96 bg-zinc-50 rounded-lg flex justify-center items-center p-12 my-6">
  <video src="/z-stripped.mp4" width={159} height={100} autoPlay muted loop playsInline className="rounded-lg" />
  <Image src="/phone.png" alt="iPhone border" width={190} height={50} className="absolute rounded" draggable={false}/>
</div>

An exciting part of working on a real mobile app is that I could access the taptic engine of the device. Having focused almost exclusively on web stacks, this was an incredible opportunity for me. I was mainly inspired by apps like [Amie](https://amie.so/), [Lapse](https://www.lapse.com/) and [ID by Amo](https://amo.co/), where haptics are part of the personality of the app. 

The main area where I included custom haptic feedback was on the power slider. As the user swipes on the slider, the haptic engine would trigger a different pattern depending on the speed of the slider. When the user reaches the top or bottom of the slider, the haptic engine would trigger a stronger and more definitive pattern. I was mesmerized by the interaction. 

Unfortunately, the demo I linked above does not include the haptic feedback (as websites do not have access to the haptic engine).

## Development

While I opted for Expo and React Native (known for its cross-platform capabilities), not all of the code worked for iOS, Android and Web out of the box. Thumbs down!

Some of the functions used by ThreeJS used for rendering instanced meshes were not fully supported on iOS. It became an issue for displaying the blade cage and blades as they are packed as instanced meshes.

As a workaround, I opted for the typical `if(Platform.OS === 'ios')...` pattern. Picking different models for the blades and the blade cage depending on the platform.

